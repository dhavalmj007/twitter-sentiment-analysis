{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_E6oV3lV.csv')\n",
    "test = pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 3 columns):\n",
      "id       31962 non-null int64\n",
      "label    31962 non-null int64\n",
      "tweet    31962 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 749.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Bidirectional, LSTM, GRU, Dense, Flatten, Input, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Embedding, SpatialDropout1D, Dropout, BatchNormalization, Conv1D, concatenate, MaxPooling2D, AveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train.tweet.values\n",
    "X_test = test.tweet.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_feature = 40000\n",
    "maxlen = 150\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 2195892 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "counter = 0\n",
    "f = open('/home/paperspace/Desktop/Kaggle/Embeddings/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    #line = line.encode('ascii', 'replace')\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    #print(word)\n",
    "#    counter = counter +1\n",
    "    #print(values[1:])\n",
    "    #if counter==2:\n",
    "        #break\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "#     text = re.sub('user', '', text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in X_train:\n",
    "    comments.append(text_to_wordlist(text, remove_stopwords=True))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in X_test:\n",
    "    test_comments.append(text_to_wordlist(text, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = Tokenizer(num_words=max_feature)\n",
    "tokenize.fit_on_texts(comments + test_comments)\n",
    "X_train = tokenize.texts_to_sequences(comments)\n",
    "X_test = tokenize.texts_to_sequences(test_comments)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenize.word_index\n",
    "nb_words = min(max_feature, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_feature: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class f1Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1, batch_size = 512)\n",
    "            y_pred[y_pred>=0.5]=1\n",
    "            y_pred[y_pred<=0.5]=0\n",
    "            score = f1_score(self.y_val, y_pred)\n",
    "            print(\"\\n F1 - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fmeasure(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_LSTM_model():\n",
    "    inp = Input((maxlen,))\n",
    "    embed = Embedding(input_dim=max_feature, output_dim=embedding_size, weights = [embedding_matrix], trainable = True)(inp)\n",
    "    x = SpatialDropout1D(0.2)(embed)\n",
    "    x = Bidirectional(LSTM(256, return_sequences = True))(embed)\n",
    "#     avg = GlobalAveragePooling1D()(x)\n",
    "#     maxpool = GlobalMaxPooling1D()(x)\n",
    "#     con = concatenate([avg, maxpool])\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=[fmeasure])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gru_model():\n",
    "    inp = Input((maxlen,))\n",
    "    embed = Embedding(input_dim=max_feature, output_dim=embedding_size, weights = [embedding_matrix], trainable = True)(inp)\n",
    "    x = SpatialDropout1D(0.25)(embed)\n",
    "    x = Bidirectional(GRU(124, return_sequences = True))(embed)\n",
    "#     avg = GlobalAveragePooling1D()(x)\n",
    "#     maxpool = GlobalMaxPooling1D()(x)\n",
    "#     con = concatenate([avg, maxpool])\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=[fmeasure])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 150, 300)          12000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 150, 248)          316200    \n",
      "_________________________________________________________________\n",
      "attention_5 (Attention)      (None, 248)               398       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                15936     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 12,332,855\n",
      "Trainable params: 12,332,727\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru = get_gru_model()\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 150, 300)          12000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 150, 512)          1140736   \n",
      "_________________________________________________________________\n",
      "attention_8 (Attention)      (None, 512)               662       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 13,174,551\n",
      "Trainable params: 13,174,423\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = get_LSTM_model()\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, train_size=0.80, random_state=233)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=111)\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "epochs = 50\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, train_size=0.80, random_state=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "# steps = int(len(X_tra)/batch_size) * epochs\n",
    "# lr_init, lr_fin = 0.001, 0.0005\n",
    "# lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "# K.set_value(model.optimizer.lr, lr_init)\n",
    "# K.set_value(model.optimizer.decay, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_gru.load_weights('bi_Gru.h5')\n",
    "model_lstm.load_weights('bi_LSTM.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold 0\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.6469 - fmeasure: 0.2776Epoch 00001: val_fmeasure improved from -inf to 0.13114, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 28s 1ms/step - loss: 0.6450 - fmeasure: 0.2759 - val_loss: 5.3211 - val_fmeasure: 0.1311\n",
      "Epoch 2/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.4937 - fmeasure: 0.3546Epoch 00002: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 950us/step - loss: 0.4888 - fmeasure: 0.3660 - val_loss: 0.2496 - val_fmeasure: nan\n",
      "Epoch 3/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.3494 - fmeasure: 0.5220Epoch 00003: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 958us/step - loss: 0.3467 - fmeasure: 0.5290 - val_loss: 0.2445 - val_fmeasure: nan\n",
      "Epoch 4/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.2471 - fmeasure: 0.6531Epoch 00004: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 960us/step - loss: 0.2453 - fmeasure: 0.6539 - val_loss: 0.2043 - val_fmeasure: nan\n",
      "Epoch 5/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1720 - fmeasure: 0.7452Epoch 00005: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 960us/step - loss: 0.1714 - fmeasure: 0.7458 - val_loss: 0.3614 - val_fmeasure: nan\n",
      "Epoch 6/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1220 - fmeasure: 0.8011Epoch 00006: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 961us/step - loss: 0.1255 - fmeasure: nan - val_loss: 0.3464 - val_fmeasure: nan\n",
      "Epoch 00006: early stopping\n",
      "17197/17197 [==============================] - 5s 277us/step\n",
      "k-fold 1\n",
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/50\n",
      "24576/25569 [===========================>..] - ETA: 1s - loss: 0.5120 - fmeasure: 0.4668Epoch 00001: val_fmeasure improved from -inf to 0.51341, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 29s 1ms/step - loss: 0.5073 - fmeasure: 0.4708 - val_loss: 0.5237 - val_fmeasure: 0.5134\n",
      "Epoch 2/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.3168 - fmeasure: 0.6246Epoch 00002: val_fmeasure improved from 0.51341 to 0.52645, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 26s 998us/step - loss: 0.3152 - fmeasure: 0.6238 - val_loss: 0.4101 - val_fmeasure: 0.5265\n",
      "Epoch 3/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.2142 - fmeasure: 0.6947Epoch 00003: val_fmeasure improved from 0.52645 to 0.58554, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 26s 998us/step - loss: 0.2133 - fmeasure: 0.6927 - val_loss: 0.3146 - val_fmeasure: 0.5855\n",
      "Epoch 4/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1568 - fmeasure: 0.7609Epoch 00004: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 991us/step - loss: 0.1559 - fmeasure: 0.7618 - val_loss: 0.2089 - val_fmeasure: 0.4976\n",
      "Epoch 5/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1011 - fmeasure: 0.8080Epoch 00005: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 988us/step - loss: 0.0998 - fmeasure: 0.8112 - val_loss: 0.1798 - val_fmeasure: 0.5549\n",
      "Epoch 6/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0848 - fmeasure: 0.8423Epoch 00006: val_fmeasure improved from 0.58554 to 0.60106, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 26s 998us/step - loss: 0.0833 - fmeasure: 0.8453 - val_loss: 0.1660 - val_fmeasure: 0.6011\n",
      "Epoch 7/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0630 - fmeasure: 0.8800Epoch 00007: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 988us/step - loss: 0.0625 - fmeasure: 0.8808 - val_loss: 0.1743 - val_fmeasure: 0.5233\n",
      "Epoch 8/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0483 - fmeasure: 0.8968Epoch 00008: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 987us/step - loss: 0.0480 - fmeasure: 0.8977 - val_loss: 0.1745 - val_fmeasure: 0.5368\n",
      "Epoch 9/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0327 - fmeasure: 0.9293Epoch 00009: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 991us/step - loss: 0.0329 - fmeasure: 0.9294 - val_loss: 0.1481 - val_fmeasure: 0.5824\n",
      "Epoch 10/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0264 - fmeasure: 0.9432Epoch 00010: val_fmeasure improved from 0.60106 to 0.67610, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 26s 1ms/step - loss: 0.0265 - fmeasure: 0.9428 - val_loss: 0.1275 - val_fmeasure: 0.6761\n",
      "Epoch 11/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0188 - fmeasure: 0.9617Epoch 00011: val_fmeasure improved from 0.67610 to 0.69926, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 26s 1000us/step - loss: 0.0208 - fmeasure: 0.9581 - val_loss: 0.1411 - val_fmeasure: 0.6993\n",
      "Epoch 12/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0188 - fmeasure: 0.9643Epoch 00012: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 992us/step - loss: 0.0186 - fmeasure: 0.9644 - val_loss: 0.1781 - val_fmeasure: 0.6436\n",
      "Epoch 13/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0103 - fmeasure: 0.9821Epoch 00013: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 990us/step - loss: 0.0106 - fmeasure: 0.9810 - val_loss: 0.2771 - val_fmeasure: 0.5788\n",
      "Epoch 14/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0234 - fmeasure: 0.9606Epoch 00014: val_fmeasure improved from 0.69926 to 0.71960, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 26s 1ms/step - loss: 0.0228 - fmeasure: 0.9612 - val_loss: 0.1513 - val_fmeasure: 0.7196\n",
      "Epoch 15/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0072 - fmeasure: 0.9897Epoch 00015: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 991us/step - loss: 0.0071 - fmeasure: 0.9898 - val_loss: 0.1941 - val_fmeasure: 0.6970\n",
      "Epoch 16/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0091 - fmeasure: 0.9829Epoch 00016: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 989us/step - loss: 0.0104 - fmeasure: 0.9810 - val_loss: 0.2573 - val_fmeasure: 0.6612\n",
      "Epoch 17/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0053 - fmeasure: 0.9916Epoch 00017: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 989us/step - loss: 0.0052 - fmeasure: 0.9917 - val_loss: 0.2080 - val_fmeasure: 0.7150\n",
      "Epoch 18/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0063 - fmeasure: 0.9885Epoch 00018: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 988us/step - loss: 0.0061 - fmeasure: 0.9890 - val_loss: 0.2304 - val_fmeasure: 0.7075\n",
      "Epoch 19/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0043 - fmeasure: 0.9931Epoch 00019: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 990us/step - loss: 0.0042 - fmeasure: 0.9933 - val_loss: 0.2364 - val_fmeasure: 0.7039\n",
      "Epoch 00019: early stopping\n",
      "17197/17197 [==============================] - 5s 283us/step\n",
      "k-fold 2\n",
      "Train on 25570 samples, validate on 6392 samples\n",
      "Epoch 1/50\n",
      "24576/25570 [===========================>..] - ETA: 1s - loss: 0.6514 - fmeasure: 0.2810Epoch 00001: val_fmeasure improved from -inf to 0.13090, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 30s 1ms/step - loss: 0.6491 - fmeasure: 0.2761 - val_loss: 4.2547 - val_fmeasure: 0.1309\n",
      "Epoch 2/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.4950 - fmeasure: 0.3601Epoch 00002: val_fmeasure improved from 0.13090 to 0.30700, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.4910 - fmeasure: 0.3708 - val_loss: 0.2377 - val_fmeasure: 0.3070\n",
      "Epoch 3/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.3587 - fmeasure: 0.5864Epoch 00003: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.3575 - fmeasure: 0.5847 - val_loss: 0.2480 - val_fmeasure: 0.1239\n",
      "Epoch 4/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.2685 - fmeasure: 0.6368Epoch 00004: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.2671 - fmeasure: 0.6396 - val_loss: 0.1930 - val_fmeasure: nan\n",
      "Epoch 5/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.2252 - fmeasure: nanEpoch 00005: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.2245 - fmeasure: nan - val_loss: 0.2192 - val_fmeasure: nan\n",
      "Epoch 6/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.2057 - fmeasure: nanEpoch 00006: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.2046 - fmeasure: nan - val_loss: 0.3060 - val_fmeasure: nan\n",
      "Epoch 7/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.1434 - fmeasure: 0.6685Epoch 00007: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.1426 - fmeasure: 0.6707 - val_loss: 0.1979 - val_fmeasure: nan\n",
      "Epoch 00007: early stopping\n",
      "17197/17197 [==============================] - 5s 285us/step\n",
      "k-fold 3\n",
      "Train on 25570 samples, validate on 6392 samples\n",
      "Epoch 1/50\n",
      "24576/25570 [===========================>..] - ETA: 1s - loss: 0.5788 - fmeasure: 0.3806Epoch 00001: val_fmeasure improved from -inf to 0.41943, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 31s 1ms/step - loss: 0.5751 - fmeasure: 0.3864 - val_loss: 0.4444 - val_fmeasure: 0.4194\n",
      "Epoch 2/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.4416 - fmeasure: 0.5087Epoch 00002: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.4393 - fmeasure: 0.5013 - val_loss: 0.2191 - val_fmeasure: 0.3604\n",
      "Epoch 3/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.2753 - fmeasure: 0.6807Epoch 00003: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.2746 - fmeasure: 0.6818 - val_loss: 0.2302 - val_fmeasure: nan\n",
      "Epoch 4/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.2023 - fmeasure: 0.7411Epoch 00004: val_fmeasure improved from 0.41943 to 0.48911, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.2019 - fmeasure: 0.7401 - val_loss: 0.1884 - val_fmeasure: 0.4891\n",
      "Epoch 5/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.1648 - fmeasure: 0.8053Epoch 00005: val_fmeasure improved from 0.48911 to 0.52836, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.1643 - fmeasure: 0.8031 - val_loss: 0.1655 - val_fmeasure: 0.5284\n",
      "Epoch 6/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.1041 - fmeasure: 0.8528Epoch 00006: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.1052 - fmeasure: 0.8483 - val_loss: 0.1677 - val_fmeasure: 0.4628\n",
      "Epoch 7/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0747 - fmeasure: 0.8664Epoch 00007: val_fmeasure improved from 0.52836 to 0.64089, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0747 - fmeasure: 0.8669 - val_loss: 0.1727 - val_fmeasure: 0.6409\n",
      "Epoch 8/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0636 - fmeasure: 0.8974Epoch 00008: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0625 - fmeasure: 0.8984 - val_loss: 0.1806 - val_fmeasure: 0.4421\n",
      "Epoch 9/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0403 - fmeasure: 0.9311Epoch 00009: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0405 - fmeasure: 0.9321 - val_loss: 0.2110 - val_fmeasure: 0.3628\n",
      "Epoch 10/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0304 - fmeasure: 0.9415Epoch 00010: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0304 - fmeasure: 0.9409 - val_loss: 0.1540 - val_fmeasure: 0.5423\n",
      "Epoch 11/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0235 - fmeasure: 0.9545Epoch 00011: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0323 - fmeasure: 0.9422 - val_loss: 0.1956 - val_fmeasure: 0.3760\n",
      "Epoch 12/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0201 - fmeasure: 0.9576Epoch 00012: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0205 - fmeasure: 0.9576 - val_loss: 0.1384 - val_fmeasure: 0.5872\n",
      "Epoch 00012: early stopping\n",
      "17197/17197 [==============================] - 5s 288us/step\n",
      "k-fold 4\n",
      "Train on 25570 samples, validate on 6392 samples\n",
      "Epoch 1/50\n",
      "24576/25570 [===========================>..] - ETA: 1s - loss: 0.5261 - fmeasure: 0.4369Epoch 00001: val_fmeasure improved from -inf to 0.30080, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 32s 1ms/step - loss: 0.5214 - fmeasure: 0.4431 - val_loss: 0.5364 - val_fmeasure: 0.3008\n",
      "Epoch 2/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.3257 - fmeasure: 0.6289Epoch 00002: val_fmeasure improved from 0.30080 to 0.43252, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.3229 - fmeasure: 0.6316 - val_loss: 0.4510 - val_fmeasure: 0.4325\n",
      "Epoch 3/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.3815 - fmeasure: 0.6669Epoch 00003: val_fmeasure improved from 0.43252 to 0.52005, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.3753 - fmeasure: 0.6667 - val_loss: 0.3136 - val_fmeasure: 0.5200\n",
      "Epoch 4/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.1691 - fmeasure: 0.7448Epoch 00004: val_fmeasure improved from 0.52005 to 0.53883, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.1675 - fmeasure: 0.7462 - val_loss: 0.2486 - val_fmeasure: 0.5388\n",
      "Epoch 5/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.1222 - fmeasure: 0.7952Epoch 00005: val_fmeasure improved from 0.53883 to 0.57191, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.1214 - fmeasure: 0.7941 - val_loss: 0.1863 - val_fmeasure: 0.5719\n",
      "Epoch 6/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0899 - fmeasure: 0.8420Epoch 00006: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0897 - fmeasure: 0.8420 - val_loss: 0.1710 - val_fmeasure: 0.4658\n",
      "Epoch 7/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0709 - fmeasure: 0.8617Epoch 00007: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0708 - fmeasure: 0.8617 - val_loss: 0.1571 - val_fmeasure: 0.5441\n",
      "Epoch 8/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0471 - fmeasure: 0.9077Epoch 00008: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0475 - fmeasure: 0.9076 - val_loss: 0.1651 - val_fmeasure: 0.4549\n",
      "Epoch 9/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0394 - fmeasure: 0.9205Epoch 00009: val_fmeasure improved from 0.57191 to 0.63614, saving model to bi_LSTM_attention.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.0391 - fmeasure: 0.9213 - val_loss: 0.1273 - val_fmeasure: 0.6361\n",
      "Epoch 10/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0294 - fmeasure: 0.9418Epoch 00010: val_fmeasure improved from 0.63614 to 0.66264, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.0297 - fmeasure: 0.9414 - val_loss: 0.2324 - val_fmeasure: 0.6626\n",
      "Epoch 11/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0352 - fmeasure: 0.9334Epoch 00011: val_fmeasure improved from 0.66264 to 0.68574, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.0344 - fmeasure: 0.9347 - val_loss: 0.1333 - val_fmeasure: 0.6857\n",
      "Epoch 12/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0186 - fmeasure: 0.9664Epoch 00012: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0186 - fmeasure: 0.9657 - val_loss: 0.1511 - val_fmeasure: 0.5929\n",
      "Epoch 13/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0142 - fmeasure: 0.9742Epoch 00013: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0143 - fmeasure: 0.9741 - val_loss: 0.1504 - val_fmeasure: 0.6795\n",
      "Epoch 14/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0160 - fmeasure: 0.9714Epoch 00014: val_fmeasure improved from 0.68574 to 0.70768, saving model to bi_LSTM_attention.hdf5\n",
      "25570/25570 [==============================] - 27s 1ms/step - loss: 0.0156 - fmeasure: 0.9720 - val_loss: 0.1343 - val_fmeasure: 0.7077\n",
      "Epoch 15/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0082 - fmeasure: 0.9852Epoch 00015: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0092 - fmeasure: 0.9840 - val_loss: 0.1797 - val_fmeasure: 0.6834\n",
      "Epoch 16/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0082 - fmeasure: 0.9859Epoch 00016: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0080 - fmeasure: 0.9859 - val_loss: 0.1950 - val_fmeasure: 0.6784\n",
      "Epoch 17/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0672 - fmeasure: 0.9294Epoch 00017: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0658 - fmeasure: 0.9298 - val_loss: 0.3502 - val_fmeasure: 0.5101\n",
      "Epoch 18/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0126 - fmeasure: 0.9741Epoch 00018: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0123 - fmeasure: 0.9751 - val_loss: 0.2540 - val_fmeasure: 0.6698\n",
      "Epoch 19/50\n",
      "24576/25570 [===========================>..] - ETA: 0s - loss: 0.0064 - fmeasure: 0.9890Epoch 00019: val_fmeasure did not improve\n",
      "25570/25570 [==============================] - 26s 1ms/step - loss: 0.0062 - fmeasure: 0.9894 - val_loss: 0.2732 - val_fmeasure: 0.7051\n",
      "Epoch 00019: early stopping\n",
      "17197/17197 [==============================] - 5s 293us/step\n"
     ]
    }
   ],
   "source": [
    "y_pre = np.zeros((17197, 1))\n",
    "k_split = 0\n",
    "for tra, val in kfold.split(X_train, y_train):\n",
    "    print('k-fold', k_split)\n",
    "    model_lstm = get_LSTM_model()\n",
    "    earlyStp = EarlyStopping(patience=5, monitor='val_fmeasure', mode = 'max', verbose=1)\n",
    "    modelChePnt = ModelCheckpoint('bi_LSTM_attention.hdf5', save_best_only=True, verbose=1, monitor='val_fmeasure', mode='max')\n",
    "    #     f1 = f1Evaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "    hist = model_lstm.fit(x = X_train[tra], y = y_train[tra], epochs = 50, validation_data=[X_train[val], y_train[val]], batch_size=batch_size, callbacks=[earlyStp, modelChePnt])\n",
    "    model_lstm.load_weights('bi_LSTM_attention.hdf5')\n",
    "    \n",
    "    y_pred_lstm = model_lstm.predict(X_test, batch_size=1024, verbose=1)\n",
    "    y_pre = y_pre + y_pred_lstm\n",
    "    k_split = k_split + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/50\n",
      "24576/25569 [===========================>..] - ETA: 1s - loss: 0.6159 - fmeasure: 0.3508Epoch 00001: val_fmeasure improved from -inf to 0.41895, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 29s 1ms/step - loss: 0.6101 - fmeasure: 0.3575 - val_loss: 0.5247 - val_fmeasure: 0.4190\n",
      "Epoch 2/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.3488 - fmeasure: 0.6107Epoch 00002: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 945us/step - loss: 0.3479 - fmeasure: 0.6109 - val_loss: 0.4252 - val_fmeasure: 0.2751\n",
      "Epoch 3/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.2424 - fmeasure: 0.6612Epoch 00003: val_fmeasure improved from 0.41895 to 0.50016, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 25s 958us/step - loss: 0.2407 - fmeasure: 0.6639 - val_loss: 0.3153 - val_fmeasure: 0.5002\n",
      "Epoch 4/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1669 - fmeasure: 0.7156Epoch 00004: val_fmeasure improved from 0.50016 to 0.57926, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 25s 966us/step - loss: 0.1667 - fmeasure: 0.7154 - val_loss: 0.3709 - val_fmeasure: 0.5793\n",
      "Epoch 5/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1191 - fmeasure: 0.7684Epoch 00005: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 25s 969us/step - loss: 0.1189 - fmeasure: 0.7694 - val_loss: 0.2009 - val_fmeasure: 0.4845\n",
      "Epoch 6/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0884 - fmeasure: 0.8272Epoch 00006: val_fmeasure improved from 0.57926 to 0.64287, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 25s 969us/step - loss: 0.0878 - fmeasure: 0.8297 - val_loss: 0.1654 - val_fmeasure: 0.6429\n",
      "Epoch 7/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0654 - fmeasure: 0.8627Epoch 00007: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 952us/step - loss: 0.0659 - fmeasure: 0.8631 - val_loss: 0.1581 - val_fmeasure: 0.6223\n",
      "Epoch 8/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0540 - fmeasure: 0.8984Epoch 00008: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 954us/step - loss: 0.0536 - fmeasure: 0.8987 - val_loss: 0.1625 - val_fmeasure: 0.5523\n",
      "Epoch 9/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0398 - fmeasure: 0.9309Epoch 00009: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 953us/step - loss: 0.0394 - fmeasure: 0.9311 - val_loss: 0.1542 - val_fmeasure: 0.6157\n",
      "Epoch 10/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0360 - fmeasure: 0.9350Epoch 00010: val_fmeasure improved from 0.64287 to 0.68910, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 25s 962us/step - loss: 0.0358 - fmeasure: 0.9356 - val_loss: 0.1460 - val_fmeasure: 0.6891\n",
      "Epoch 11/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0243 - fmeasure: 0.9614Epoch 00011: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 951us/step - loss: 0.0238 - fmeasure: 0.9614 - val_loss: 0.1553 - val_fmeasure: 0.6683\n",
      "Epoch 12/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0207 - fmeasure: 0.9714Epoch 00012: val_fmeasure improved from 0.68910 to 0.69563, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 25s 963us/step - loss: 0.0205 - fmeasure: 0.9711 - val_loss: 0.1539 - val_fmeasure: 0.6956\n",
      "Epoch 13/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0193 - fmeasure: 0.9694Epoch 00013: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 952us/step - loss: 0.0209 - fmeasure: 0.9679 - val_loss: 0.1362 - val_fmeasure: 0.6472\n",
      "Epoch 14/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0158 - fmeasure: 0.9740Epoch 00014: val_fmeasure improved from 0.69563 to 0.70536, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 25s 962us/step - loss: 0.0155 - fmeasure: 0.9745 - val_loss: 0.1640 - val_fmeasure: 0.7054\n",
      "Epoch 15/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0127 - fmeasure: 0.9808Epoch 00015: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 952us/step - loss: 0.0126 - fmeasure: 0.9807 - val_loss: 0.2370 - val_fmeasure: 0.7040\n",
      "Epoch 16/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0095 - fmeasure: 0.9875Epoch 00016: val_fmeasure improved from 0.70536 to 0.70955, saving model to bi_LSTM_attention.hdf5\n",
      "25569/25569 [==============================] - 25s 961us/step - loss: 0.0092 - fmeasure: 0.9877 - val_loss: 0.2203 - val_fmeasure: 0.7096\n",
      "Epoch 17/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1766 - fmeasure: 0.8447Epoch 00017: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 955us/step - loss: 0.1713 - fmeasure: 0.8486 - val_loss: 0.9734 - val_fmeasure: 0.1767\n",
      "Epoch 18/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0204 - fmeasure: 0.9664Epoch 00018: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 953us/step - loss: 0.0199 - fmeasure: 0.9668 - val_loss: 0.4906 - val_fmeasure: 0.5689\n",
      "Epoch 19/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0098 - fmeasure: 0.9867Epoch 00019: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 952us/step - loss: 0.0100 - fmeasure: 0.9864 - val_loss: 2.7267 - val_fmeasure: 0.1260\n",
      "Epoch 20/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0107 - fmeasure: 0.9812Epoch 00020: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 952us/step - loss: 0.0111 - fmeasure: 0.9805 - val_loss: 0.2563 - val_fmeasure: 0.6610\n",
      "Epoch 21/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0089 - fmeasure: 0.9883Epoch 00021: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 24s 949us/step - loss: 0.0088 - fmeasure: 0.9885 - val_loss: 0.3331 - val_fmeasure: 0.6797\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "earlyStp = EarlyStopping(patience=5, monitor='val_fmeasure', mode = 'max', verbose=1)\n",
    "modelChePnt = ModelCheckpoint('bi_LSTM_attention.hdf5', save_best_only=True, verbose=1, monitor='val_fmeasure', mode='max')\n",
    "# f1 = f1Evaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model_lstm.fit(x = X_tra, y = y_tra, epochs = epochs, validation_data=[X_val, y_val], batch_size=batch_size, callbacks=[earlyStp, modelChePnt])\n",
    "model_lstm.load_weights('bi_LSTM_attention.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25569 samples, validate on 6393 samples\n",
      "Epoch 1/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.1153 - fmeasure: 0.7921Epoch 00001: val_fmeasure improved from -inf to 0.56265, saving model to bi_GRU_attention.hdf5\n",
      "25569/25569 [==============================] - 12s 475us/step - loss: 0.1155 - fmeasure: 0.7906 - val_loss: 0.1784 - val_fmeasure: 0.5627\n",
      "Epoch 2/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0801 - fmeasure: 0.8394Epoch 00002: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 422us/step - loss: 0.0797 - fmeasure: 0.8401 - val_loss: 0.1863 - val_fmeasure: 0.4600\n",
      "Epoch 3/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0598 - fmeasure: 0.8755Epoch 00003: val_fmeasure improved from 0.56265 to 0.66631, saving model to bi_GRU_attention.hdf5\n",
      "25569/25569 [==============================] - 11s 430us/step - loss: 0.0593 - fmeasure: 0.8767 - val_loss: 0.2003 - val_fmeasure: 0.6663\n",
      "Epoch 4/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0429 - fmeasure: 0.9120Epoch 00004: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 428us/step - loss: 0.0427 - fmeasure: 0.9122 - val_loss: 0.1979 - val_fmeasure: 0.5667\n",
      "Epoch 5/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0363 - fmeasure: 0.9242Epoch 00005: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 421us/step - loss: 0.0363 - fmeasure: 0.9252 - val_loss: 0.1711 - val_fmeasure: 0.6136\n",
      "Epoch 6/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0258 - fmeasure: 0.9496Epoch 00006: val_fmeasure improved from 0.66631 to 0.70366, saving model to bi_GRU_attention.hdf5\n",
      "25569/25569 [==============================] - 11s 442us/step - loss: 0.0254 - fmeasure: 0.9503 - val_loss: 0.2399 - val_fmeasure: 0.7037\n",
      "Epoch 7/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0262 - fmeasure: 0.9545Epoch 00007: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 431us/step - loss: 0.0261 - fmeasure: 0.9547 - val_loss: 0.2027 - val_fmeasure: 0.6529\n",
      "Epoch 8/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0144 - fmeasure: 0.9788Epoch 00008: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 414us/step - loss: 0.0143 - fmeasure: 0.9787 - val_loss: 0.2277 - val_fmeasure: 0.6413\n",
      "Epoch 9/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0115 - fmeasure: 0.9794Epoch 00009: val_fmeasure improved from 0.70366 to 0.71806, saving model to bi_GRU_attention.hdf5\n",
      "25569/25569 [==============================] - 11s 438us/step - loss: 0.0114 - fmeasure: 0.9794 - val_loss: 0.1806 - val_fmeasure: 0.7181\n",
      "Epoch 10/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0094 - fmeasure: 0.9854Epoch 00010: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 416us/step - loss: 0.0093 - fmeasure: 0.9854 - val_loss: 0.2031 - val_fmeasure: 0.7172\n",
      "Epoch 11/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0319 - fmeasure: 0.9346Epoch 00011: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 417us/step - loss: 0.0315 - fmeasure: 0.9354 - val_loss: 0.3057 - val_fmeasure: 0.5255\n",
      "Epoch 12/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0158 - fmeasure: 0.9716Epoch 00012: val_fmeasure improved from 0.71806 to 0.73025, saving model to bi_GRU_attention.hdf5\n",
      "25569/25569 [==============================] - 11s 436us/step - loss: 0.0157 - fmeasure: 0.9717 - val_loss: 0.1660 - val_fmeasure: 0.7302\n",
      "Epoch 13/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0131 - fmeasure: 0.9802Epoch 00013: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 426us/step - loss: 0.0149 - fmeasure: 0.9759 - val_loss: 1.4117 - val_fmeasure: 0.1498\n",
      "Epoch 14/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0139 - fmeasure: 0.9786Epoch 00014: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 414us/step - loss: 0.0137 - fmeasure: 0.9787 - val_loss: 0.2435 - val_fmeasure: 0.6823\n",
      "Epoch 15/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0128 - fmeasure: 0.9799Epoch 00015: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 434us/step - loss: 0.0124 - fmeasure: 0.9807 - val_loss: 0.2784 - val_fmeasure: 0.6854\n",
      "Epoch 16/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0067 - fmeasure: 0.9896Epoch 00016: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 420us/step - loss: 0.0065 - fmeasure: 0.9898 - val_loss: 0.2494 - val_fmeasure: 0.7060\n",
      "Epoch 17/50\n",
      "24576/25569 [===========================>..] - ETA: 0s - loss: 0.0103 - fmeasure: 0.9856Epoch 00017: val_fmeasure did not improve\n",
      "25569/25569 [==============================] - 11s 430us/step - loss: 0.0103 - fmeasure: 0.9854 - val_loss: 0.2811 - val_fmeasure: 0.7020\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "earlyStp = EarlyStopping(patience=5, monitor='val_fmeasure', mode = 'max', verbose=1)\n",
    "modelChePnt = ModelCheckpoint('bi_GRU_attention.hdf5', save_best_only=True, verbose=1, monitor='val_fmeasure', mode='max')\n",
    "# f1 = f1Evaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model_gru.fit(x = X_tra, y = y_tra, epochs = epochs, validation_data=[X_val, y_val], batch_size=batch_size, callbacks=[earlyStp, modelChePnt])\n",
    "model_gru.load_weights('bi_GRU_attention.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17197/17197 [==============================] - 4s 245us/step\n",
      "17197/17197 [==============================] - 2s 97us/step\n"
     ]
    }
   ],
   "source": [
    "#ensembling\n",
    "y_pred_lstm = model_lstm.predict(X_test, batch_size=1024, verbose=1)\n",
    "y_pred_gru = model_gru.predict(X_test, batch_size=1024, verbose=1)\n",
    "\n",
    "y_pre = (y_pred_lstm + y_pred_gru)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17197/17197 [==============================] - 2s 126us/step\n"
     ]
    }
   ],
   "source": [
    "model_gru.load_weights('bi_GRU_attention.hdf5')\n",
    "y_pre = model_gru.predict(X_test, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17197/17197 [==============================] - 5s 270us/step\n"
     ]
    }
   ],
   "source": [
    "model_lstm.load_weights('bi_LSTM_attention.hdf5')\n",
    "y_pre = model_lstm.predict(X_test, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission_3Mm4cJo.csv')\n",
    "submission['id'] = test.id\n",
    "submission['label'] = y_pre/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "thres = 0.5\n",
    "submission['label'].loc[submission['label']>=thres]=1\n",
    "submission['label'].loc[submission['label']<thres]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label\n",
       "0  31963      0\n",
       "1  31964      1\n",
       "2  31965      0\n",
       "3  31966      0\n",
       "4  31967      0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['label'] = submission.label.astype('int')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16272\n",
       "1      925\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' white supremacists want everyone see new  birds movie  heres'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 17197})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred>=0.1]=0\n",
    "y_pred[y_pred<=0.1]=1\n",
    "Counter(y_pred.astype('int')[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>off to concelebrate at the #albanpilgrimage fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>@user let the scum-baggery begin.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>the white establishment can't have blk fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>@user why not @user mocked obama for being bla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet\n",
       "17    18      1                             retweet if you agree! \n",
       "52    53      0  off to concelebrate at the #albanpilgrimage fo...\n",
       "53    54      0             @user let the scum-baggery begin..... \n",
       "68    69      1  the white establishment can't have blk fol...\n",
       "114  115      1  @user why not @user mocked obama for being bla..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train[0:170]\n",
    "gen = data['tweet'].values\n",
    "gen = tokenize.texts_to_sequences(gen)\n",
    "gen = sequence.pad_sequences(gen, maxlen=maxlen, padding='post')\n",
    "\n",
    "pred = np.round(model_lstm.predict(gen))\n",
    "\n",
    "data.loc[(pred.T != data['label'].values).reshape(-1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-966a470cde6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m68\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "num = 68\n",
    "pred[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-17a10de1c6d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.iloc[num].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e9dda4451e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scum-baggery'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_index' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings_index.get('scum-baggery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
